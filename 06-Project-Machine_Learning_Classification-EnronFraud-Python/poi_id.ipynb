{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnara\\Anaconda3\\envs\\DAND\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Miscellanious functions '''\n",
    "\n",
    "def computeFraction(salary, bonus):\n",
    "    \"\"\" Computing how much EXTRA salaries you will receive in bonus \"\"\"    \n",
    "    invalid_data = salary == 'NaN' or bonus == 'NaN'\n",
    "    fraction = 0. if invalid_data else float(bonus) / float(salary)\n",
    "    return fraction\n",
    "\n",
    "def add_bonus_by_salary(data_dict):\n",
    "    \"\"\" I'm creating a new feature merging mixing salary and bonus\"\"\"\n",
    "    submit_dict = {}\n",
    "    for name in data_dict:\n",
    "        data_point = data_dict[name]        \n",
    "        salary = data_point[\"salary\"]\n",
    "        bonus = data_point[\"bonus\"]\n",
    "        bonus_fraction = computeFraction(salary,bonus)        \n",
    "        data_point[\"bonus_extra_salary\"] = bonus_fraction   \n",
    "\n",
    "def clean_observed_outliers(data_dict,key):\n",
    "    data_dict.pop(key,0)    \n",
    "    \n",
    "def show_metrics(method,pred, labels_test, importances = None):    \n",
    "    ac = accuracy_score(pred, labels_test)\n",
    "    pr = precision_score(labels_test, pred, average=\"binary\")\n",
    "    re = recall_score(labels_test, pred, average=\"binary\")\n",
    "    f1 = f1_score(labels_test, pred, average=\"binary\")    \n",
    "    print \"METHOD:\", method\n",
    "    print \"\\t\",\"Features used:\", features_list\n",
    "    print \"\\t\",\"Accuracy:\",ac\n",
    "    print \"\\t\",\"Precision:\",pr\n",
    "    print \"\\t\",\"Recall:\",re\n",
    "    print \"\\t\",\"F1:\",f1\n",
    "    if (importances is not None):\n",
    "        print \"Importances:\"\n",
    "        i = 0    \n",
    "        for imp in importances:    \n",
    "            if imp > 0 and i<len(features_list)-1:                    \n",
    "                i = i + 1                 \n",
    "                print \"\\t\",features_list[i],\":\",imp                   \n",
    "    \n",
    "def make_meshgrid(x, y, h=0.1):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"            \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_classifier_relation(ax, features_train,labels_train,x_name,y_name,model,h):\n",
    "    \n",
    "    # Take two features and plot the predicted classification\n",
    "    X = pd.DataFrame(features_train, columns=features_list[1:])[[x_name,y_name]]\n",
    "    y = pd.DataFrame(labels_train)[0]\n",
    "    \n",
    "    \n",
    "    if model == 'GaussianNB':\n",
    "        estimators = [('Feature Selection', PCA(svd_solver='randomized',n_components=2)),\n",
    "                      ('Classification', GaussianNB())]\n",
    "    elif model == 'DecisionTreeClassifier':\n",
    "        estimators = [('Classification', DecisionTreeClassifier(min_samples_split=2, max_depth=15))]\n",
    "    else:\n",
    "        estimators = [('Feature Scaling', MinMaxScaler()),\n",
    "                      ('Feature Selection', PCA(svd_solver='randomized',n_components=2)),\n",
    "                      ('Classification', SVC(C=2450, gamma=5.7, kernel='rbf'))]\n",
    "        \n",
    "    clf = Pipeline(estimators)\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    X0, X1 = X[x_name], X[y_name]\n",
    "    xx, yy = make_meshgrid(X0, X1,h)\n",
    "           \n",
    "    plot_contours(ax, clf, xx, yy,cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel(x_name)\n",
    "    ax.set_ylabel(y_name)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',                    \n",
    "                 'bonus_extra_salary',                 \n",
    "                 'shared_receipt_with_poi',                 \n",
    "                 'deferred_income',\n",
    "                 'exercised_stock_options',                 \n",
    "                 'long_term_incentive',\n",
    "                 'bonus',\n",
    "                 'salary',\n",
    "                 'deferral_payments'\n",
    "                 ]\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "### Task 2: Remove outliers\n",
    "clean_observed_outliers(data_dict,'TOTAL')\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "add_bonus_by_salary(my_dataset) #Adding new basic feature Fraction of extra salaries from bonus to and from POI\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL FEATURES:\n",
      "['bonus_extra_salary', 'shared_receipt_with_poi', 'deferred_income', 'exercised_stock_options', 'long_term_incentive', 'bonus', 'salary', 'deferral_payments']\n",
      "\n",
      "INTELLIGENT FEATURE SELECTION:\n",
      "SelectPercentile:(percentaje=50) ['deferred_income', 'exercised_stock_options', 'bonus', 'salary']\n",
      "SelectKBest(k=5): ['bonus_extra_salary', 'deferred_income', 'exercised_stock_options', 'bonus', 'salary']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif\n",
    "\n",
    "### Used feature selection thru SelectBestK and SelectPercentile methods.\n",
    "### I used f_classif since I used features and labes and classification models.\n",
    "\n",
    "X = features\n",
    "y = labels\n",
    "K = 5\n",
    "P = 50\n",
    "\n",
    "print \"ORIGINAL FEATURES:\"\n",
    "print features_list[1:]\n",
    "print\n",
    "print \"INTELLIGENT FEATURE SELECTION:\"\n",
    "selector1 = SelectPercentile(f_classif,percentile=P)\n",
    "selector1.fit(X, y)\n",
    "mask1 = selector1.get_support()\n",
    "new_features1 = []\n",
    "for bool, feature in zip(mask1,features_list[1:]):\n",
    "    if bool:\n",
    "        new_features1.append(feature)\n",
    "print \"SelectPercentile:(percentaje=50)\", new_features1\n",
    "\n",
    "selector2 = SelectKBest(f_classif,k=K)\n",
    "selector2.fit(X, y)\n",
    "mask2 = selector2.get_support()\n",
    "new_features2 = []\n",
    "for bool, feature in zip(mask2, features_list[1:]):\n",
    "    if bool:\n",
    "        new_features2.append(feature)\n",
    "print \"SelectKBest(k=5):\", new_features2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'reduce_dim__n_components': 5} with a score of 0.66\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi'] + new_features2\n",
    "\n",
    "# We then store the split instance into cv and use it in our GridSearchCV.\n",
    "cv = StratifiedShuffleSplit(n_splits=10,random_state=42)\n",
    "parameters = [{'reduce_dim__n_components': [2, 3, 4,5]}]\n",
    "pipe = Pipeline([('reduce_dim',PCA(svd_solver='randomized')),\n",
    "                 ('classification', GaussianNB())])\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv = cv, scoring='average_precision').fit(features, labels)\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio pc: [ 0.90914715  0.05151069  0.02277208  0.0119676   0.00398829]\n",
      "First pc: [  9.27452548e-08   2.69619877e-05  -1.65022376e-02   9.90867379e-01\n",
      "   5.71004220e-02   1.17682796e-01   1.82643945e-02   2.15922754e-02]\n",
      "Second pc: [  2.53703621e-06   6.16930482e-04  -1.26771619e-01  -1.33056515e-01\n",
      "   2.90345199e-01   9.29548824e-01   8.89167998e-02   9.97847313e-02]\n",
      "METHOD: GaussianNB\n",
      "\tFeatures used: ['poi', 'bonus_extra_salary', 'deferred_income', 'exercised_stock_options', 'bonus', 'salary']\n",
      "\tAccuracy: 0.928571428571\n",
      "\tPrecision: 0.4\n",
      "\tRecall: 1.0\n",
      "\tF1: 0.571428571429\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "#I switched to a basic train_test_split to improve validation time.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf1 = Pipeline([('reduce_dim',PCA(svd_solver='randomized', n_components=5)),\n",
    "                 ('classification', GaussianNB())])\n",
    "\n",
    "clf1.fit(features_train, labels_train)\n",
    "\n",
    "print \"Explained variance ratio pc:\", clf1.named_steps['reduce_dim'].explained_variance_ratio_\n",
    "print \"First pc:\", clf1.named_steps['reduce_dim'].components_[0]\n",
    "print \"Second pc:\",clf1.named_steps['reduce_dim'].components_[1]\n",
    "\n",
    "pred1 = clf1.predict(features_test) \n",
    "show_metrics(\"GaussianNB\",pred1,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHOD: DecisionTreeClassifier\n",
      "\tFeatures used: ['poi', 'bonus_extra_salary', 'deferred_income', 'exercised_stock_options', 'bonus', 'salary']\n",
      "\tAccuracy: 0.880952380952\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1: 0.0\n",
      "Importances:\n",
      "\tbonus_extra_salary : 0.118453223464\n",
      "\tdeferred_income : 0.22915023289\n",
      "\texercised_stock_options : 0.170392776321\n",
      "\tbonus : 0.190346350892\n",
      "\tsalary : 0.112268518519\n"
     ]
    }
   ],
   "source": [
    "#I switched to a basic train_test_split to improve validation time.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(min_samples_split=2)\n",
    "clf2.fit(features_train, labels_train)\n",
    "\n",
    "pred2 = clf2.predict(features_test) \n",
    "show_metrics(\"DecisionTreeClassifier\",pred2,labels_test,clf2.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHOD: SVC\n",
      "\tFeatures used: ['poi', 'bonus_extra_salary', 'deferred_income', 'exercised_stock_options', 'bonus', 'salary']\n",
      "\tAccuracy: 0.833333333333\n",
      "\tPrecision: 0.142857142857\n",
      "\tRecall: 0.5\n",
      "\tF1: 0.222222222222\n"
     ]
    }
   ],
   "source": [
    "#I switched to a basic train_test_split to improve validation time.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "#Scaling features previous to dimensionality reduction using PCA\n",
    "clf3 = Pipeline([('feature_scaling',MinMaxScaler()),\n",
    "                 ('reduce_dim', PCA(svd_solver='randomized', n_components=2)),\n",
    "                 ('classification', SVC(kernel='rbf',C=2450,gamma=5.7))]).fit(features_train, labels_train)\n",
    "\n",
    "pred3 = clf3.predict(features_test)\n",
    "show_metrics(\"SVC\",pred3,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf1\n",
    "pred = pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, ax1 = plt.subplots(2, 3,figsize=(10,5))\\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\\nplot_classifier_relation(ax1[0, 0], features_train,labels_train,features_list[1],features_list[2],'GaussianNB',0.1)\\nplot_classifier_relation(ax1[0, 1], features_train,labels_train,features_list[1],features_list[3],'GaussianNB',10)\\nplot_classifier_relation(ax1[0, 2], features_train,labels_train,features_list[1],features_list[4],'GaussianNB',10)\\nplot_classifier_relation(ax1[1, 0], features_train,labels_train,features_list[2],features_list[3],'GaussianNB',100)\\nplot_classifier_relation(ax1[1, 1], features_train,labels_train,features_list[2],features_list[4],'GaussianNB',100)\\nplot_classifier_relation(ax1[1, 2], features_train,labels_train,features_list[3],features_list[4],'GaussianNB',5000)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting Naive bayes desicion boudary\n",
    "'''\n",
    "f, ax1 = plt.subplots(2, 3,figsize=(10,5))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "plot_classifier_relation(ax1[0, 0], features_train,labels_train,features_list[1],features_list[2],'GaussianNB',0.1)\n",
    "plot_classifier_relation(ax1[0, 1], features_train,labels_train,features_list[1],features_list[3],'GaussianNB',10)\n",
    "plot_classifier_relation(ax1[0, 2], features_train,labels_train,features_list[1],features_list[4],'GaussianNB',10)\n",
    "plot_classifier_relation(ax1[1, 0], features_train,labels_train,features_list[2],features_list[3],'GaussianNB',100)\n",
    "plot_classifier_relation(ax1[1, 1], features_train,labels_train,features_list[2],features_list[4],'GaussianNB',100)\n",
    "plot_classifier_relation(ax1[1, 2], features_train,labels_train,features_list[3],features_list[4],'GaussianNB',5000)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, ax2 = plt.subplots(2, 3,figsize=(10,5))\\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\\nplot_classifier_relation(ax2[0, 0], features_train,labels_train,features_list[1],features_list[2],'DecisionTreeClassifier',0.1)\\nplot_classifier_relation(ax2[0, 1], features_train,labels_train,features_list[1],features_list[3],'DecisionTreeClassifier',5)\\nplot_classifier_relation(ax2[0, 2], features_train,labels_train,features_list[1],features_list[4],'DecisionTreeClassifier',5)\\nplot_classifier_relation(ax2[1, 0], features_train,labels_train,features_list[2],features_list[3],'DecisionTreeClassifier',100)\\nplot_classifier_relation(ax2[1, 1], features_train,labels_train,features_list[2],features_list[4],'DecisionTreeClassifier',100)\\nplot_classifier_relation(ax2[1, 2], features_train,labels_train,features_list[3],features_list[4],'DecisionTreeClassifier',5000)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting Decision Tree Classifier\n",
    "'''\n",
    "f, ax2 = plt.subplots(2, 3,figsize=(10,5))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "plot_classifier_relation(ax2[0, 0], features_train,labels_train,features_list[1],features_list[2],'DecisionTreeClassifier',0.1)\n",
    "plot_classifier_relation(ax2[0, 1], features_train,labels_train,features_list[1],features_list[3],'DecisionTreeClassifier',5)\n",
    "plot_classifier_relation(ax2[0, 2], features_train,labels_train,features_list[1],features_list[4],'DecisionTreeClassifier',5)\n",
    "plot_classifier_relation(ax2[1, 0], features_train,labels_train,features_list[2],features_list[3],'DecisionTreeClassifier',100)\n",
    "plot_classifier_relation(ax2[1, 1], features_train,labels_train,features_list[2],features_list[4],'DecisionTreeClassifier',100)\n",
    "plot_classifier_relation(ax2[1, 2], features_train,labels_train,features_list[3],features_list[4],'DecisionTreeClassifier',5000)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf, ax3 = plt.subplots(2, 3,figsize=(10,5))\\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\\nplot_classifier_relation(ax3[0, 0], features_train,labels_train,features_list[1],features_list[2],'SVC',0.5)\\nplot_classifier_relation(ax3[0, 1], features_train,labels_train,features_list[1],features_list[3],'SVC',10)\\nplot_classifier_relation(ax3[0, 2], features_train,labels_train,features_list[1],features_list[4],'SVC',10)\\nplot_classifier_relation(ax3[1, 0], features_train,labels_train,features_list[2],features_list[3],'SVC',100)\\nplot_classifier_relation(ax3[1, 1], features_train,labels_train,features_list[2],features_list[4],'SVC',100)\\nplot_classifier_relation(ax3[1, 2], features_train,labels_train,features_list[3],features_list[4],'SVC',50000)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting Naive bayes desicion boudary\n",
    "'''\n",
    "f, ax3 = plt.subplots(2, 3,figsize=(10,5))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "plot_classifier_relation(ax3[0, 0], features_train,labels_train,features_list[1],features_list[2],'SVC',0.5)\n",
    "plot_classifier_relation(ax3[0, 1], features_train,labels_train,features_list[1],features_list[3],'SVC',10)\n",
    "plot_classifier_relation(ax3[0, 2], features_train,labels_train,features_list[1],features_list[4],'SVC',10)\n",
    "plot_classifier_relation(ax3[1, 0], features_train,labels_train,features_list[2],features_list[3],'SVC',100)\n",
    "plot_classifier_relation(ax3[1, 1], features_train,labels_train,features_list[2],features_list[4],'SVC',100)\n",
    "plot_classifier_relation(ax3[1, 2], features_train,labels_train,features_list[3],features_list[4],'SVC',50000)\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
